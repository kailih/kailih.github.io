<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta http-equiv="refresh" content="3; url=https://medium.com/@kailihuang/which-trick-works-best-in-gnn-analyzed-on-arxiv-citation-dataset-e9f29ce07e04"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Which Trick Works Best in GNN? | Kaili Huang </title> <meta name="author" content="Kaili Huang"> <meta name="description" content="In this blog, we reproduced the work of " residual network and embedding usage new tricks of node classification with graph convolutional networks. deliver a thorough introduction analysis. in detail we implemented the attention several commonly used to challenge task on ogbn-arxiv dataset. our experiments show satisfactory results this at same time explored effectiveness different proposed or introduced paper. from found that node2vec embeddings label c achieved noticeable improvements while changing structure gat leveraging self-kd barely increases accuracy.> <meta name="keywords" content="kaili,kaili-huang,stanford-university,tsinghua-university,microsoft,bytedance,machine-learning,deep-learning,artificial-intelligence,natural-language-processing,AI,NLP,ML,DL"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_1.png?9079a81f2031466668e4a405d8515fec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kailih.github.io/projects/cs224w_21fall/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kaili</span> Huang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Which Trick Works Best in GNN?</h1> <p class="post-description">In this blog, we reproduced the work of "Residual Network and Embedding Usage - New Tricks of Node Classification with Graph Convolutional Networks." and deliver a thorough introduction and analysis. In detail, we implemented the Graph Attention Network and several commonly used tricks to challenge the node classification task on the ogbn-arxiv dataset. Our experiments show satisfactory results on this dataset. At the same time, we explored the effectiveness of different tricks proposed or introduced in the paper. From our experiments, we found that Node2Vec embeddings, label usage, and C&amp;S achieved noticeable improvements on the task while changing the network structure of GAT and leveraging self-KD barely increases the accuracy.</p> </header> <article> <hr> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!--
  See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
  https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
--&gt;

  
    &lt;source
      class="responsive-img-srcset"
      
        srcset="https://cdn-images-1.medium.com/max/1600/1*j12WBNry4YUdmMBobEOuhQ-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*j12WBNry4YUdmMBobEOuhQ-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*j12WBNry4YUdmMBobEOuhQ-1400.webp 1400w,"
        type="image/webp"
      
      
        sizes="95vw"
      
    &gt;
  

&lt;img
  src="https://cdn-images-1.medium.com/max/1600/1*j12WBNry4YUdmMBobEOuhQ.png"
  
    class="img-fluid rounded z-depth-1"
  
  
    width="100%"
  
  
    height="auto"
  
  
  
  
    title="example image"
  
  
  
    loading="lazy"
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <div class="caption"> ogbn-arxiv dataset (All figures in this blog are made by our team except those explicitly cites the sources) </div> <p>Each paper comes with a feature vector, publication year, title, and abstract. The feature vectors are computed by first calculating the word embeddings over the MAG corpus with the skip-gram model [2] and then averaging over each paper’s title and abstract. The publication years range from 1971 to 2020.</p> <p>The dataset comes with manually annotated labels and proposes the task of predicting 40 categories of arXiv CS papers, e.g., cs.AI (Artificial Intelligence), cs.LG (Machine Learning), cs.OS (Operating Systems). The metric is classification accuracy.</p> <p>Given the goal of predicting recent papers’ areas based on historical data, papers published until 2017 are used for training, while those published in 2018 are used for validating and those published since 2019 are for testing.</p> <h3 id="gat-graph-attention-network">GAT (Graph Attention Network)</h3> <p>As a classical Graph Neural Network (GNN)[3], Graph Attention Network (GAT) [4] can be decomposed into two components as other GNNs: (1) Message (2) Aggregation. Initially, each node is assigned a node feature, which in our dataset is the paper’s feature vector. In a network that consists <em>L</em> GNN layers, at each layer <em>l</em> (<em>1, 2, …, L</em>), each node first calculates its own “message” with a message function, and then every node updates its node feature by aggregating the “messages” from all its neighbor nodes as well as itself. The Message and Aggregation steps can be concluded as below:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*EtsWfX972MpYzpmJyYpJPw@2x-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*EtsWfX972MpYzpmJyYpJPw@2x-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*EtsWfX972MpYzpmJyYpJPw@2x-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*EtsWfX972MpYzpmJyYpJPw@2x.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Message Step, h is the node feature, N(v) is the set of node v’s neighbor nodes, MSG is a function and it can be a neural network </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*5RWFfORm1sh6KaBNq4zSSA-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*5RWFfORm1sh6KaBNq4zSSA-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*5RWFfORm1sh6KaBNq4zSSA-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*5RWFfORm1sh6KaBNq4zSSA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Aggregate Step, AGG is a function such as MEAN, MAX, SUM </div> <p>GAT, in addition, applies an extra self-attention mechanism to GNN. At the Aggregation step, the “message” of neighbor nodes are aggregated with respect to an “attention score”. For a central node <em>A</em> and one of its neighbor node <em>B</em>, an attention score <em>α</em> between <em>A</em> and <em>B</em> is computed and it determines the “weight” of neighbor node <em>B</em>’s “message” when central node <em>A</em> is doing Aggregation.</p> <p>This attention score can be seen as a factor of importance. As supposed to the average aggregation where the weights are all <em>1/N</em> (where <em>N</em> denotes the number of neighbors node <em>A</em> has), <em>α</em> of <em>A</em> and <em>B</em> can be higher or lower than <em>1/N</em>, indicating a larger or smaller impact that the neighbor node <em>B</em> has on the central node <em>A</em>.</p> <p>The number of <em>α</em> is computed by taking the node features of central nodes and neighbor nodes, and doing a sequence of transforms as below (here <em>i</em> corresponds to node <em>A</em>, and <em>j</em> corresponds to node <em>B</em>)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*b_4tis5cRqoPVGCIPXM2BA-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*b_4tis5cRqoPVGCIPXM2BA-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*b_4tis5cRqoPVGCIPXM2BA-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*b_4tis5cRqoPVGCIPXM2BA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> i is the central node, j is i’s neighbor node. W is a parameter matrix. a is a mechanism [4] </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*zn-PH1hH7SjC9mEaQY8OSA-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*zn-PH1hH7SjC9mEaQY8OSA-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*zn-PH1hH7SjC9mEaQY8OSA-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*zn-PH1hH7SjC9mEaQY8OSA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ni is the neighbor node set of node i [4] </div> <p>By using a single-layer feedforward neural network as the mechanism <em>a</em>, and applying the LeakyReLU nonlinearity, we have the expression for <em>α</em>:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*8UmW43ZkomrEJ_udbEZrzw-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*8UmW43ZkomrEJ_udbEZrzw-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*8UmW43ZkomrEJ_udbEZrzw-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*8UmW43ZkomrEJ_udbEZrzw.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> a is a parameter vector, W is a parameter matrix [4] </div> <p>Without loss of generality, we could draw an example of the neighborhood of node <em>A</em>:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*v4tV6y6kj6HqXxa7pqaLeA-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*v4tV6y6kj6HqXxa7pqaLeA-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*v4tV6y6kj6HqXxa7pqaLeA-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*v4tV6y6kj6HqXxa7pqaLeA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>And then the computation of attention scores can be visualized as:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*O7ug5qqk1rrEQr4Q6AipwA-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*O7ug5qqk1rrEQr4Q6AipwA-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*O7ug5qqk1rrEQr4Q6AipwA-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*O7ug5qqk1rrEQr4Q6AipwA.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The computation process of the attention score α of A and B </div> <p>We further adopt multi-head attention [5] to stabilize the learning process of self-attention. We use <em>K</em> independent attention mechanisms (or “heads”) to separately compute the updated node feature representations and then concatenate these representations. Assume we are using a Two-Head Attention, an example aggregation process will look like below, where the solid line and dotted line correspond to two different attention “head”, and the width of lines indicate attention scores.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*J_x1FOPF1QZvuRz4FR8AFQ-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*J_x1FOPF1QZvuRz4FR8AFQ-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*J_x1FOPF1QZvuRz4FR8AFQ-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*J_x1FOPF1QZvuRz4FR8AFQ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Aggregation step of node A with Two-Head Attention mechanism </div> <p>The base model pipeline we applied is shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://cdn-images-1.medium.com/max/1600/1*4BWgzj1hJCKBmuPaYM9edQ-480.webp 480w,https://cdn-images-1.medium.com/max/1600/1*4BWgzj1hJCKBmuPaYM9edQ-800.webp 800w,https://cdn-images-1.medium.com/max/1600/1*4BWgzj1hJCKBmuPaYM9edQ-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://cdn-images-1.medium.com/max/1600/1*4BWgzj1hJCKBmuPaYM9edQ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Aggregation step of node A with Two-Head Attention mechanism </div> <p>Base GAT model pipeline</p> <h3 id="tricks">Tricks</h3> <p>Upon the base GAT model, we also added several tricks that were found useful in GNNs. It’s helpful to know about these commonly used tricks, and when you’re building a GNN of your own, give them a try and they might boost your model’s performance. We will walk you through these tricks and introduce their effect on the overall performance later.</p> <h4 id="enrich-node-features-with-node2vec-embeddings">Enrich node features with Node2Vec embeddings</h4> <p>Node features provided by the dataset itself do not contain information about the graph structure. Therefore, it sounds like a good idea to enrich the node features with structural knowledge. Node2Vec is a popular method to learn node features from the graph structure without the use of any external supervision [6].</p> <p>Through simulating biased random walks, Node2Vec learns multi-hop neighborhood structure effectively. As Node2Vec is not the main focus of this tutorial, we will stop here and leave interested readers to the <a href="https://arxiv.org/abs/1607.00653" rel="external nofollow noopener" target="_blank">Node2Vec</a> paper.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*7m2BR6FA6tR3niv5j6uhiw.png" alt=""></p> <p>Random walk of different styles [6]</p> <p>We directly leverage Node2Vec implemented in PyTorch Geometric to train the embeddings. For implementation details, you can read the source code of <a href="https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/node2vec.html" rel="external nofollow noopener" target="_blank">Node2Vec in PyTorch Geometric</a>.</p> <p>To use the learned features (vectors of length 128), we concatenate them with the node features provided by the dataset (also vectors of length 128) and feed them into the network.</p> <p>Here’s a code snippet showing our implementation of Node2Vec:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*bHWMUfDDi94D2gxp1x7ELg.png" alt=""></p> <p>Node2Vec implementation code snippet</p> <h4 id="gat-variant">GAT Variant</h4> <p>GAT is a classical network structure for GNN, and there have been some variants. Bag of tricks [7] borrowed the idea from Graph Convolutional Network (GCN) [8] with <em>Symmetric Normalized Adjacency Matrix</em> and applied it to GAT.</p> <p>Say we write the update rules of traditional GAT in matrix form (for simplicity, we discuss the single-head attention scenario):</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*nSxApmuebwqAbr165c-Aqw.png" alt=""></p> <p>For clarity we write down the definition of the attention matrix:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*32XWPlXPNrqYqYcbomn_QQ.png" alt=""></p> <p>Then Bag of tricks added a residual block to the layer and also normalized the attention matrix:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*W7MIMlGg_zK9g9hT59eYxQ.png" alt=""></p> <p>Where:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*8CpUb1KBezF3NkUkPbGqtQ.png" alt=""></p> <p>In order words, we first add self-loops to the graph and then calculate the <a href="https://en.wikipedia.org/wiki/Degree_matrix" rel="external nofollow noopener" target="_blank">degree matrix</a> (<em>D</em>) and attention matrix. (By default, <em>A</em> and <em>D</em> do not contain self-loops)</p> <p>To implement the GAT variant, we create a class inherited from PyTorch Geometric’s GATConv and rewrite the forward function, as shown in the code snippet below.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*rzSMW-MktIjFW9NhE6Kdug.png" alt=""></p> <p>GAT variant implementation code snippet</p> <h4 id="label-usage">Label Usage</h4> <p>In the node classification task, we have access to some nodes’ ground-truth labels and need to predict unknown nodes. Label Usage [7] is a popular trick which explicitly uses label information when predicting the test node labels.</p> <p>Label usage is a data augmentation method. By leveraging the information of labels, the classifier could gather more correct information during the training process. When the training accuracy is below 100%, the label information of the misclassified samples is not contained in the model, despite the fact that they can provide additional information. At the same time, these misclassified samples could further mislead its neighbor, so adding correct information in the training process is very important.</p> <p>In the label usage algorithm, <em>f_θ</em> denotes the GNN. We concatenate the input feature and the one-hot ground truth vector together as the input (if no ground truth, then the one-hot vector is replaced by an all-zero vector). After getting the predicted outputs (soft labels), we assigned these soft labels to unlabelled nodes.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*FYAUsOD35Shq-XxxIB5jFQ.png" alt=""></p> <p>Label Usage Algorithm, cite from paper [7]</p> <h4 id="correct-and-smooth">Correct and Smooth</h4> <p>Correct and Smooth[9] (C&amp;S) is a recent state-of-the-art classification method, which uses the graph structure to do post-processing. It follows three steps:</p> <ol> <li>Train a base predictor and use it to predict soft labels for all nodes.</li> </ol> <p>The base predictor could be any type of model that generates a soft label for each node. For example, in our task, we choose GAT as our base model to classify arXiv papers.</p> <ol> <li>Correct step:</li> </ol> <p>In this step, we calculate the training residual, which is the difference between a ground-truth label and the predicted soft label. If a node doesn’t have a label (which means it is in the test set), the residual is a zero vector. Then we diffuse the training residual along with the graph structure. The diffusion step will use the following formula:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*3ep-c4n_NUWb6t1zbSUWQQ.png" alt=""></p> <p><em>E^(t)</em> is the training residual during <em>t</em>-th diffusion iteration, <em>A</em> is the diffusion matrix, and <em>α</em> is a parameter</p> <p>The output of the correct step is a combination of soft label and diffused error.</p> <p>Here’s a GIF to illustrate the correct step:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*a69kNZrttkXw-1GACQC9Lw.gif" alt=""></p> <p>Illustration of the Correct Step</p> <ol> <li>Smooth step:</li> </ol> <p>In the smooth step, we use the ground-truth label and output of the correct step as input. Then it diffuses the label along with the graph structure. The label diffusion process can be described with the following formula:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*Il61jcl1NwHt7i6slqE4wA.png" alt=""></p> <p><em>Z^(t)</em> is the node label during <em>t</em>-th diffusion iteration, <em>A</em> is the diffusion matrix, and <em>α</em> is a parameter</p> <p>Here’s a GIF to illustrate the smooth step:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*F8kA6QHwtZEllQ57xRbw5w.gif" alt=""></p> <p>Illustration of the Smooth Step</p> <p>Here is a pipeline showing the C&amp;S post-processing method.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*4b4p1PHQydcTS5uIRI506g.png" alt=""></p> <p>Pipeline of C&amp;S</p> <p>Please refer to the code snippet for implement details:</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*zoV3WcUIQy34X7dEMlxAKQ.png" alt=""></p> <p>C&amp;S implement code snippet</p> <hr> <p>Let’s take a break here and see how far we’ve got! Till now, our base model has added 4 tricks (add node2vec embeddings, modify GAT architecture, label usage, C&amp;S). The current pipeline of our model is shown below. The orange box represents the modification of the GAT model, the yellow box represents adding node2vec embeddings, the green box represents label usage, and the blue box represents C&amp;S.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*v4eVJMaamoZooqJc4NbzLg.png" alt=""></p> <p>Our model pipeline after adding <em>4</em> tricks</p> <hr> <h4 id="self-kd">Self-KD</h4> <p>Knowledge Distillation (KD) is a model compression method in which a small model is trained to mimic a pre-trained, larger model (or ensemble of models). A common approach is to induce an extra loss with the predictions from a stronger model as labels. Self-KD is a trick that originated from the idea of KD. Sometimes, if we first train a model and then use it as the teacher model to train a second model in a KD fashion, the second model would perform better.</p> <hr> <p>Below is an illustration of our complete model after adding all the tricks we mentioned above. And let’s get started on the experiments!</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*3aKryC8HGOeEkIRZhY3utg.png" alt=""></p> <p>The complete model after adding all tricks mentioned above</p> <hr> <h3 id="experiments">Experiments</h3> <p>We use <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a> and the <a href="https://pytorch-geometric.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">PyTorch Geometric</a> library to implement our model and train it with Google Colab. We keep our model structure, learning schedule, etc., mostly the same as the setting of [10], though due to memory constraint we have to reduce the hidden layer size of GAT.</p> <h4 id="training-process">Training Process</h4> <p>One example training process is shown in the following picture. During the training process, the training loss gradually decreases, while the test accuracy fluctuates. The test accuracy of this example training process is about 0.73. Compared to the top methods on the <a href="https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxiv" rel="external nofollow noopener" target="_blank">OGB leaderboard</a> with the same dataset, our model achieves similar high accuracy.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*9d_Pce0iz3Wf8eO6ZatF2g.png" alt=""></p> <p>Training process</p> <p>We further visualized the training process on the GAT model in the first 500 epochs. After using <a href="https://lvdmaaten.github.io/tsne/" rel="external nofollow noopener" target="_blank">t-SNE</a>, a technique for dimensionality reduction, to map the embedding to 2-dimensional space, the GIF shows how the hidden embedding after the first GAT layer changes during the training process. The widths of edges show the attention scores in GAT. From epoch 0 to epoch 499, the nodes belonging to the same class gradually group together. At the same time, the attention mechanism generates some important attention edges during this process.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*R89SumE8o1gAgjL3UfHShg.gif" alt=""></p> <p>The GIF showing our training process</p> <h3 id="results">Results</h3> <p>We designed our experiments in an incremental way. In the chart below, every row consists of one more trick of the previous row, while the first row is the GAT baseline setting with no bells and whistles.</p> <p>Our experimental results are shown in the table.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*ioZaBIk7oXKfHlJIK5dAPA.png" alt=""></p> <p>Validation and test results of our models (figures in the parentheses show the improvements from baseline)</p> <p>The comparison of different models can also be seen clearly in the figure below.</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*vjKg8wG7Pul5F_7VtTHm1g.png" alt=""></p> <p>Experiment result of different model settings</p> <p>We can observe that:</p> <ul> <li>Training with Node2Vec embeddings improves both validation accuracy and test accuracy by about 1 point. Though not shown in this chart, the training process also converged noticeably faster.</li> <li>Adding residual connections and normalizing the attention matrix together brought tiny improvement. Since we only used 3 layers of GAT, residual connections may not be as useful as they are in deep models.</li> <li>Label usage and C&amp;S further boosted the performance by 0.3% and 0.5%, respectively. These two tricks both involve leveraging ground truth labels, and they may have similar effects on the learning process — using neighbors with ground truth to guide prediction.</li> <li>At last, experiments show self-KD brought no performance gain. We must point out that, averaging the outcome of 5 runs is still not statistically stable enough to claim that self-KD is not helpful in our setting, but chances are high that they are not as significant as other tricks.</li> </ul> <p><strong>The final result compared to leaderboard data:</strong></p> <p>Our results are lower than the numbers on the leaderboard (~0.5%), probably because of two reasons:</p> <ul> <li>The author used one V100 GPU for training, which supports more parameters in the hidden layer of GAT (64 v.s. 256)</li> <li>Randomness</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>In this blog, we reproduced the work of [10] and deliver a thorough introduction and analysis. In detail, we implemented the Graph Attention Network [4] and several commonly used tricks to challenge the node classification task on the ogbn-arxiv dataset. Our experiments show satisfactory results on this dataset. At the same time, we explored the effectiveness of different tricks proposed or introduced in the paper. From our experiments, we found that Node2Vec embeddings, label usage, and C&amp;S achieved noticeable improvements on the task while changing the network structure of GAT and leveraging self-KD barely increases the accuracy.</p> <p>Going from top to bottom of the ogbn-arxiv leaderboard we can observe an interesting fact: leading methods use a bunch of tricks. Tricks may not bring consistent gain across all tasks, but if you are not satisfied with your model, just give them a try and wait for a surprise!</p> <h3 id="references">References</h3> <ol> <li>Wang, K., Shen, Z., Huang, C., Wu, C.-H., Dong, Y., &amp; Kanakia, A. (2020). Microsoft Academic Graph: When experts are not enough. <em>Quantitative Science Studies</em>, <em>1</em>(1), 396–413. <a href="https://doi.org/10.1162/qss_a_00021" rel="external nofollow noopener" target="_blank">https://doi.org/10.1162/qss_a_00021</a> </li> <li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. In <em>NIPS</em> (pp. 3111–3119) . Curran Associates, Inc. .</li> <li>Scarselli, F., Gori, M., Ah Chung Tsoi, Hagenbuchner, M., &amp; Monfardini, G. (2009). The Graph Neural Network Model. <em>IEEE Transactions on Neural Networks</em>, <em>20</em>(1), 61–80. <a href="https://doi.org/10.1109/tnn.2008.2005605" rel="external nofollow noopener" target="_blank">https://doi.org/10.1109/tnn.2008.2005605</a>‌</li> <li>Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P. &amp; Bengio, Y. (2017). Graph Attention Networks. <em>ICLR 2018</em>, .</li> <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. &amp; Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan &amp; R. Garnett (ed.), <em>Advances in Neural Information Processing Systems 30</em> (pp. 5998–6008) . Curran Associates, Inc. .</li> <li>Grover, A. &amp; Leskovec, J. (2016). node2vec: Scalable Feature Learning for Networks.. <em>CoRR</em>, abs/1607.00653.</li> <li>Wang, Y., Jin, J., Zhang, W., Yu, Y., Zhang, Z., &amp; Wipf, D. (2021). Bag of Tricks for Node Classification with Graph Neural Networks. <em>arXiv preprint arXiv:2103.13355</em>.</li> <li>Kipf, T. N. &amp; Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks. <em>5th International Conference on Learning Representations</em>.</li> <li>Huang, Q., He, H., Singh, A., Lim, S. N., &amp; Benson, A. R. (2020). Combining label propagation and simple models out-performs graph neural networks. <em>arXiv preprint arXiv:2010.13993</em>.</li> <li>Chi, H., Wang, Y., Hao, Q., &amp; Xia, H. (2021). Residual Network and Embedding Usage: New Tricks of Node Classification with Graph Convolutional Networks. <em>arXiv preprint arXiv:2105.08330</em>. –&gt;</li> </ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kaili Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>